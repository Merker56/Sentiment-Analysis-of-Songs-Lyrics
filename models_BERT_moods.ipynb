{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models_BERT_moods.ipynb","provenance":[{"file_id":"1Pf2SZpe0BAhnGfoG0-Cy9WjmaMEb_Hjb","timestamp":1575656305349},{"file_id":"1PMSdRCHKv68QDrk-1zEoqCriYax5ykK7","timestamp":1575611826998},{"file_id":"1_vnnyAOl9ujxv7siO3z2JejSHvBkFkWA","timestamp":1575485089711}],"collapsed_sections":["iRtocS0NeTTw"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"l2kBNKlwjvx2","colab_type":"text"},"source":["# **Models_BERT**"]},{"cell_type":"markdown","metadata":{"id":"apFDFUQWj9G7","colab_type":"text"},"source":["## Mount drive and import libraries"]},{"cell_type":"code","metadata":{"id":"OM_3Jeunnnna","colab_type":"code","outputId":"2e1fa605-eefc-44c0-91f0-1e11f29951b4","executionInfo":{"status":"ok","timestamp":1575773373440,"user_tz":300,"elapsed":25053,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Mount a drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RjTR8WZTQFMX","colab_type":"code","colab":{}},"source":["# Import libraries\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import random\n","import collections\n","import os\n","# SK-learn libraries for learning.\n","# from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","# SK-learn libraries for evaluation.\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","from sklearn.metrics import classification_report\n","\n","# NLP libraries\n","import nltk, re, pprint, string\n","# from nltk import word_tokenize\n","# nltk.download('punkt')\n","\n","# Set seed\n","random.seed(8)\n","np.random.seed(seed=13)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s7LnU5DBkoDx","colab_type":"text"},"source":["## Import data"]},{"cell_type":"code","metadata":{"id":"U9PR61iqkwFg","colab_type":"code","colab":{}},"source":["# Path to the file MoodyLyrics4Q_raw.pkl\n","path_lyrics = \"/content/drive/My Drive/Project/2_clean_data/data_lyrics_clean.pkl\"\n","# If David runs, use this path \"/content/drive/My Drive/Fall 2019/W266/Project/2_clean_data/data_lyrics_clean.pkl\"\n","# Unpickle the dataset in pandas dataframe\n","df = pd.read_pickle(path_lyrics)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-9sTw4O_lOf0"},"source":["## EDA"]},{"cell_type":"code","metadata":{"id":"QfGdN-FQm0Yr","colab_type":"code","outputId":"4393de9a-bff8-412b-ee63-4d4f809eb6a0","executionInfo":{"status":"ok","timestamp":1575773376161,"user_tz":300,"elapsed":27524,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["df.head(5)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ml_index</th>\n","      <th>mer_index</th>\n","      <th>artist</th>\n","      <th>title</th>\n","      <th>mood</th>\n","      <th>mood_pn</th>\n","      <th>lyrics</th>\n","      <th>lyrics_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>George Michael</td>\n","      <td>I Want Your Sex</td>\n","      <td>happy</td>\n","      <td>pos</td>\n","      <td>There's things that you guess, And things that...</td>\n","      <td>375</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>Rob Zombie</td>\n","      <td>Pussy Liquor</td>\n","      <td>angry</td>\n","      <td>neg</td>\n","      <td>Baby: Gimme a B, Gimme a A, Gimme a B, Gimme a...</td>\n","      <td>353</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3.0</td>\n","      <td>NaN</td>\n","      <td>Katatonia</td>\n","      <td>12</td>\n","      <td>sad</td>\n","      <td>neg</td>\n","      <td>Black theatre of love, Violet thatncers cast t...</td>\n","      <td>88</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.0</td>\n","      <td>NaN</td>\n","      <td>Bing Crosby</td>\n","      <td>Swinging On A Star</td>\n","      <td>happy</td>\n","      <td>pos</td>\n","      <td>Would you like to swing on a star, Carry moonb...</td>\n","      <td>285</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>NaN</td>\n","      <td>Ludacris</td>\n","      <td>Get Back</td>\n","      <td>angry</td>\n","      <td>neg</td>\n","      <td>Hands up! Hands up, Here's another one, And a ...</td>\n","      <td>835</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ml_index  ... lyrics_len\n","0       1.0  ...        375\n","1       2.0  ...        353\n","2       3.0  ...         88\n","3       4.0  ...        285\n","4       5.0  ...        835\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kTVzAyKelOf7","outputId":"9496a657-f29d-43b5-d7ab-39d31f01cb62","executionInfo":{"status":"ok","timestamp":1575773376163,"user_tz":300,"elapsed":27492,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Explore the dataset\n","df.tail(5)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ml_index</th>\n","      <th>mer_index</th>\n","      <th>artist</th>\n","      <th>title</th>\n","      <th>mood</th>\n","      <th>mood_pn</th>\n","      <th>lyrics</th>\n","      <th>lyrics_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2569</th>\n","      <td>NaN</td>\n","      <td>MT0033391277</td>\n","      <td>Simon Garfunkel</td>\n","      <td>April Come She Will</td>\n","      <td>relaxed</td>\n","      <td>pos</td>\n","      <td>, April come she will, When streams are ripe a...</td>\n","      <td>67</td>\n","    </tr>\n","    <tr>\n","      <th>2570</th>\n","      <td>NaN</td>\n","      <td>MT0033733581</td>\n","      <td>Sinead O'Connor</td>\n","      <td>Door Peep</td>\n","      <td>relaxed</td>\n","      <td>pos</td>\n","      <td>Door peep shall not enter this a holy land, Wh...</td>\n","      <td>171</td>\n","    </tr>\n","    <tr>\n","      <th>2571</th>\n","      <td>NaN</td>\n","      <td>MT0033994043</td>\n","      <td>Rosemary Clooney</td>\n","      <td>Tenderly</td>\n","      <td>relaxed</td>\n","      <td>pos</td>\n","      <td>The evening breeze caressed the trees tenderly...</td>\n","      <td>70</td>\n","    </tr>\n","    <tr>\n","      <th>2572</th>\n","      <td>NaN</td>\n","      <td>MT0034992204</td>\n","      <td>John Denver</td>\n","      <td>Farewell Andromeda (Welcome to My Morning)</td>\n","      <td>relaxed</td>\n","      <td>pos</td>\n","      <td>Welcome to my morning, Welcome to my thaty, I'...</td>\n","      <td>172</td>\n","    </tr>\n","    <tr>\n","      <th>2573</th>\n","      <td>NaN</td>\n","      <td>MT0035119161</td>\n","      <td>Elton John</td>\n","      <td>Dixie Lily</td>\n","      <td>relaxed</td>\n","      <td>pos</td>\n","      <td>Showboat coming up the river, See her lanterns...</td>\n","      <td>129</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      ml_index  ... lyrics_len\n","2569       NaN  ...         67\n","2570       NaN  ...        171\n","2571       NaN  ...         70\n","2572       NaN  ...        172\n","2573       NaN  ...        129\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"LgH16lBNHcBe","colab_type":"text"},"source":["## Train/Test split"]},{"cell_type":"markdown","metadata":{"id":"6ciquhXpH99j","colab_type":"text"},"source":["We will define a train/test split ratio"]},{"cell_type":"code","metadata":{"id":"vj8qYNaWHmuT","colab_type":"code","colab":{}},"source":["# test ratio portion over all observations\n","test_ratio = 0.2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Cs-psB0IO-3","colab_type":"text"},"source":["### Pos/neg mood"]},{"cell_type":"code","metadata":{"id":"nGZh_1LaJPHD","colab_type":"code","colab":{}},"source":["# Split the data into random train and test subsets for pos/neg mood prediction\n","X_pn_train, X_pn_test, y_pn_train, y_pn_test = train_test_split(df['lyrics'], df['mood_pn'], test_size=test_ratio, random_state=8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsMVeh3VdSNY","colab_type":"code","outputId":"e35ec777-957e-4c94-cc0d-9e6882f2a706","executionInfo":{"status":"ok","timestamp":1575773376168,"user_tz":300,"elapsed":27321,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["# Check train and test split\n","print(f'Train size: {len(y_pn_train)}')\n","print(f'Test size: {len(y_pn_test)}')\n","\n","\n","# Check balance in random subsets\n","print(f'\\nPositive mood in train: {np.sum(y_pn_train==\"pos\")}')\n","print(f'Negative mood in train: {np.sum(y_pn_train==\"neg\")}')\n","print(f'\\nPositive mood in test: {np.sum(y_pn_test==\"pos\")}')\n","print(f'Negative mood in test: {np.sum(y_pn_test==\"neg\")}')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Train size: 2059\n","Test size: 515\n","\n","Positive mood in train: 1005\n","Negative mood in train: 1054\n","\n","Positive mood in test: 250\n","Negative mood in test: 265\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iRtocS0NeTTw","colab_type":"text"},"source":["### 4Q mood"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R9oFdokVedV_","colab":{}},"source":["# Split the data into random train and test subsets for 4Q mood prediction\n","X_m_train, X_m_test, y_m_train, y_m_test = train_test_split(df['lyrics'], df['mood'], test_size=0.2, random_state=8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yHsa0dQ8gkpD","outputId":"558d9235-7ce6-4db2-fab3-0dbc1d71b5d3","executionInfo":{"status":"ok","timestamp":1575773376170,"user_tz":300,"elapsed":26991,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["# Check balance in random subsets: train\n","index_train = list(y_m_train.index)\n","df.loc[index_train, :].groupby(by='mood').count()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ml_index</th>\n","      <th>mer_index</th>\n","      <th>artist</th>\n","      <th>title</th>\n","      <th>mood_pn</th>\n","      <th>lyrics</th>\n","      <th>lyrics_len</th>\n","    </tr>\n","    <tr>\n","      <th>mood</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>angry</th>\n","      <td>370</td>\n","      <td>159</td>\n","      <td>529</td>\n","      <td>529</td>\n","      <td>529</td>\n","      <td>529</td>\n","      <td>529</td>\n","    </tr>\n","    <tr>\n","      <th>happy</th>\n","      <td>372</td>\n","      <td>163</td>\n","      <td>535</td>\n","      <td>535</td>\n","      <td>535</td>\n","      <td>535</td>\n","      <td>535</td>\n","    </tr>\n","    <tr>\n","      <th>relaxed</th>\n","      <td>354</td>\n","      <td>116</td>\n","      <td>470</td>\n","      <td>470</td>\n","      <td>470</td>\n","      <td>470</td>\n","      <td>470</td>\n","    </tr>\n","    <tr>\n","      <th>sad</th>\n","      <td>353</td>\n","      <td>172</td>\n","      <td>525</td>\n","      <td>525</td>\n","      <td>525</td>\n","      <td>525</td>\n","      <td>525</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         ml_index  mer_index  artist  title  mood_pn  lyrics  lyrics_len\n","mood                                                                    \n","angry         370        159     529    529      529     529         529\n","happy         372        163     535    535      535     535         535\n","relaxed       354        116     470    470      470     470         470\n","sad           353        172     525    525      525     525         525"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d8KcHlhwmsRk","outputId":"58854c10-ff02-4a67-9ace-cd6ed43090ec","executionInfo":{"status":"ok","timestamp":1575773376171,"user_tz":300,"elapsed":26984,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["# Check balance in random subsets:test\n","index_test = list(y_m_test.index)\n","df.loc[index_test, :].groupby(by='mood').count()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ml_index</th>\n","      <th>mer_index</th>\n","      <th>artist</th>\n","      <th>title</th>\n","      <th>mood_pn</th>\n","      <th>lyrics</th>\n","      <th>lyrics_len</th>\n","    </tr>\n","    <tr>\n","      <th>mood</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>angry</th>\n","      <td>98</td>\n","      <td>45</td>\n","      <td>143</td>\n","      <td>143</td>\n","      <td>143</td>\n","      <td>143</td>\n","      <td>143</td>\n","    </tr>\n","    <tr>\n","      <th>happy</th>\n","      <td>84</td>\n","      <td>48</td>\n","      <td>132</td>\n","      <td>132</td>\n","      <td>132</td>\n","      <td>132</td>\n","      <td>132</td>\n","    </tr>\n","    <tr>\n","      <th>relaxed</th>\n","      <td>86</td>\n","      <td>32</td>\n","      <td>118</td>\n","      <td>118</td>\n","      <td>118</td>\n","      <td>118</td>\n","      <td>118</td>\n","    </tr>\n","    <tr>\n","      <th>sad</th>\n","      <td>90</td>\n","      <td>32</td>\n","      <td>122</td>\n","      <td>122</td>\n","      <td>122</td>\n","      <td>122</td>\n","      <td>122</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         ml_index  mer_index  artist  title  mood_pn  lyrics  lyrics_len\n","mood                                                                    \n","angry          98         45     143    143      143     143         143\n","happy          84         48     132    132      132     132         132\n","relaxed        86         32     118    118      118     118         118\n","sad            90         32     122    122      122     122         122"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"YK0cB9ntFgRc","colab_type":"text"},"source":["## Models"]},{"cell_type":"markdown","metadata":{"id":"jOet-0Rf5sX2","colab_type":"text"},"source":["The data is already splitted in train and test datasets."]},{"cell_type":"markdown","metadata":{"id":"zUpEWIUJtetr","colab_type":"text"},"source":["### BERT"]},{"cell_type":"code","metadata":{"id":"ZI-cMnMEtiQs","colab_type":"code","outputId":"a3f3a21b-4e3a-4a07-b55b-167ae7902abc","executionInfo":{"status":"ok","timestamp":1575773380938,"user_tz":300,"elapsed":31739,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["#BERT TF Library\n","!pip install bert-tensorflow"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |████▉                           | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1E7wfR8ztor6","colab_type":"code","outputId":"c55d7c0d-b618-49ae-e844-9308eebe9d5e","executionInfo":{"status":"ok","timestamp":1575773382272,"user_tz":300,"elapsed":33062,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PNs6b8q07pIw","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afiWftFnt77G","colab_type":"text"},"source":["BERT Parameters\n","\n","DATA_COLUMN is the column of information to be evaluated. In our case, our lyrics. LABEL_COLUMN is the column to be predicted. In our case, we will start with pos/neg and then the mood.\n"]},{"cell_type":"code","metadata":{"id":"EN8btdUct8Uj","colab_type":"code","colab":{}},"source":["DATA_COLUMN = 'lyrics'\n","LABEL_COLUMN = 'mood' #change to mood later\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = ['angry', 'sad', 'happy', 'relaxed'] # change to ['angry', 'sad', 'happy', 'relaxed']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hGkdBgDh01Tr","colab_type":"text"},"source":["BERT Pre-processing\n","\n","*   `text_a` is the text we want to classify, which in this case, is the `lyrics` field in our Dataframe.\n","*   `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is text_b a translation of text_a? Is text_b an answer to the question asked by text_a?). This doesn't apply to our task, so we can leave `text_b` blank.\n","*   `label` is the `label` for our example, i.e. True, False"]},{"cell_type":"code","metadata":{"id":"1yvXgueouoOd","colab_type":"code","colab":{}},"source":["## Datasets\n","## Pos/Neg - X_pn_train, X_pn_test, y_pn_train, y_pn_test  \n","## Mood - X_m_train, X_m_test, y_m_train, y_m_test\n","train, test = train_test_split(df, test_size=0.2, random_state=8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6KUplQc_dpP","colab_type":"code","colab":{}},"source":["train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zSNFLtYd8Eok","colab_type":"text"},"source":["## Text Pre-processing\n","\n","A large portion of the data was been cleaned up prior to this step, but BERT has specific formats that it is expecting for the model to work correctly."]},{"cell_type":"code","metadata":{"id":"UyJrxgM58lpX","colab_type":"code","outputId":"74e90dcf-f8c8-4177-bcdd-71da1bc59402","executionInfo":{"status":"ok","timestamp":1575773393598,"user_tz":300,"elapsed":44331,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# This is a path to an uncased (all lowercase) version of BERT\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","def create_tokenizer_from_hub_module():\n","  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","  with tf.Graph().as_default():\n","    bert_module = hub.Module(BERT_MODEL_HUB)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","      \n","  return bert.tokenization.FullTokenizer(\n","      vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","tokenizer = create_tokenizer_from_hub_module()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"MRlc1piSAfwe","colab_type":"code","outputId":"5ad93584-e42e-4aeb-b819-94628be710d5","executionInfo":{"status":"ok","timestamp":1575773404180,"user_tz":300,"elapsed":54896,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# We'll set sequences to be at most 128 tokens long.\n","MAX_SEQ_LENGTH = 128\n","# Convert our train and test features to InputFeatures that BERT understands.\n","train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 2059\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 2059\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] right here we go now , a sociology lecture , with a bit of psychology , a bit of ne ##uro ##logy , a bit of fuck - all - ol ##ogy , no fun . right no fun , my babe no fun , no fun , my babe no fun , fun to be alone , alone and by myself , fun to be alone , in love with nobody else . no fun , my babe no fun , no fun , my babe no fun , fun to be alone , walking by myself , fun to be alone , in love with no friends of mine . maybe going out , or maybe stay at home , maybe call somebody on [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] right here we go now , a sociology lecture , with a bit of psychology , a bit of ne ##uro ##logy , a bit of fuck - all - ol ##ogy , no fun . right no fun , my babe no fun , no fun , my babe no fun , fun to be alone , alone and by myself , fun to be alone , in love with nobody else . no fun , my babe no fun , no fun , my babe no fun , fun to be alone , walking by myself , fun to be alone , in love with no friends of mine . maybe going out , or maybe stay at home , maybe call somebody on [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2157 2182 2057 2175 2085 1010 1037 11507 8835 1010 2007 1037 2978 1997 6825 1010 1037 2978 1997 11265 10976 6483 1010 1037 2978 1997 6616 1011 2035 1011 19330 15707 1010 2053 4569 1012 2157 2053 4569 1010 2026 11561 2053 4569 1010 2053 4569 1010 2026 11561 2053 4569 1010 4569 2000 2022 2894 1010 2894 1998 2011 2870 1010 4569 2000 2022 2894 1010 1999 2293 2007 6343 2842 1012 2053 4569 1010 2026 11561 2053 4569 1010 2053 4569 1010 2026 11561 2053 4569 1010 4569 2000 2022 2894 1010 3788 2011 2870 1010 4569 2000 2022 2894 1010 1999 2293 2007 2053 2814 1997 3067 1012 2672 2183 2041 1010 2030 2672 2994 2012 2188 1010 2672 2655 8307 2006 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2157 2182 2057 2175 2085 1010 1037 11507 8835 1010 2007 1037 2978 1997 6825 1010 1037 2978 1997 11265 10976 6483 1010 1037 2978 1997 6616 1011 2035 1011 19330 15707 1010 2053 4569 1012 2157 2053 4569 1010 2026 11561 2053 4569 1010 2053 4569 1010 2026 11561 2053 4569 1010 4569 2000 2022 2894 1010 2894 1998 2011 2870 1010 4569 2000 2022 2894 1010 1999 2293 2007 6343 2842 1012 2053 4569 1010 2026 11561 2053 4569 1010 2053 4569 1010 2026 11561 2053 4569 1010 4569 2000 2022 2894 1010 3788 2011 2870 1010 4569 2000 2022 2894 1010 1999 2293 2007 2053 2814 1997 3067 1012 2672 2183 2041 1010 2030 2672 2994 2012 2188 1010 2672 2655 8307 2006 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: angry (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: angry (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] a black sheep boy revolves , over canyon ##s and waterfalls , a black sheep boy dissolve ##s , in sy ##ring ##e or in shower stall . says , \" there ' s plenty of time to make you mine tonight \" , says , \" there ' s plenty of time to make you mine \" . says , \" there ' s plenty of ways to know you ' re not dying , all right , hell , there ' s plenty of light still left in your eyes , in your eyes \" . a black sheep boy grows horns , breathing smoke through his microphone , the air ##wave ##s stretch and they groan , bleeding , birth ##ing his black [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] a black sheep boy revolves , over canyon ##s and waterfalls , a black sheep boy dissolve ##s , in sy ##ring ##e or in shower stall . says , \" there ' s plenty of time to make you mine tonight \" , says , \" there ' s plenty of time to make you mine \" . says , \" there ' s plenty of ways to know you ' re not dying , all right , hell , there ' s plenty of light still left in your eyes , in your eyes \" . a black sheep boy grows horns , breathing smoke through his microphone , the air ##wave ##s stretch and they groan , bleeding , birth ##ing his black [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1037 2304 8351 2879 19223 1010 2058 8399 2015 1998 24236 1010 1037 2304 8351 2879 21969 2015 1010 1999 25353 4892 2063 2030 1999 6457 13498 1012 2758 1010 1000 2045 1005 1055 7564 1997 2051 2000 2191 2017 3067 3892 1000 1010 2758 1010 1000 2045 1005 1055 7564 1997 2051 2000 2191 2017 3067 1000 1012 2758 1010 1000 2045 1005 1055 7564 1997 3971 2000 2113 2017 1005 2128 2025 5996 1010 2035 2157 1010 3109 1010 2045 1005 1055 7564 1997 2422 2145 2187 1999 2115 2159 1010 1999 2115 2159 1000 1012 1037 2304 8351 2879 7502 11569 1010 5505 5610 2083 2010 15545 1010 1996 2250 16535 2015 7683 1998 2027 13001 1010 9524 1010 4182 2075 2010 2304 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1037 2304 8351 2879 19223 1010 2058 8399 2015 1998 24236 1010 1037 2304 8351 2879 21969 2015 1010 1999 25353 4892 2063 2030 1999 6457 13498 1012 2758 1010 1000 2045 1005 1055 7564 1997 2051 2000 2191 2017 3067 3892 1000 1010 2758 1010 1000 2045 1005 1055 7564 1997 2051 2000 2191 2017 3067 1000 1012 2758 1010 1000 2045 1005 1055 7564 1997 3971 2000 2113 2017 1005 2128 2025 5996 1010 2035 2157 1010 3109 1010 2045 1005 1055 7564 1997 2422 2145 2187 1999 2115 2159 1010 1999 2115 2159 1000 1012 1037 2304 8351 2879 7502 11569 1010 5505 5610 2083 2010 15545 1010 1996 2250 16535 2015 7683 1998 2027 13001 1010 9524 1010 4182 2075 2010 2304 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: sad (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: sad (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] oh you ' re so filthy , and i am gorgeous . when you ' re walking down the street , and a man tries to get your business , and the people that you meet , want to open you up like christmas , you have to wrap your fuzzy with a big red bow , ain ##gt no sum bitch going to treat me like a ho , i ' m a class ##y honey kiss ##y hug ##gy love ##y dove ##y ghetto princess . because you ' re filthy filthy , o ##oh , and i ' m gorgeous gorgeous , because you ' re filthy filthy , o ##oh , and i ' m gorgeous gorgeous , you ' re disgusting [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] oh you ' re so filthy , and i am gorgeous . when you ' re walking down the street , and a man tries to get your business , and the people that you meet , want to open you up like christmas , you have to wrap your fuzzy with a big red bow , ain ##gt no sum bitch going to treat me like a ho , i ' m a class ##y honey kiss ##y hug ##gy love ##y dove ##y ghetto princess . because you ' re filthy filthy , o ##oh , and i ' m gorgeous gorgeous , because you ' re filthy filthy , o ##oh , and i ' m gorgeous gorgeous , you ' re disgusting [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2821 2017 1005 2128 2061 18294 1010 1998 1045 2572 9882 1012 2043 2017 1005 2128 3788 2091 1996 2395 1010 1998 1037 2158 5363 2000 2131 2115 2449 1010 1998 1996 2111 2008 2017 3113 1010 2215 2000 2330 2017 2039 2066 4234 1010 2017 2031 2000 10236 2115 18001 2007 1037 2502 2417 6812 1010 7110 13512 2053 7680 7743 2183 2000 7438 2033 2066 1037 7570 1010 1045 1005 1049 1037 2465 2100 6861 3610 2100 8549 6292 2293 2100 10855 2100 17276 4615 1012 2138 2017 1005 2128 18294 18294 1010 1051 11631 1010 1998 1045 1005 1049 9882 9882 1010 2138 2017 1005 2128 18294 18294 1010 1051 11631 1010 1998 1045 1005 1049 9882 9882 1010 2017 1005 2128 19424 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2821 2017 1005 2128 2061 18294 1010 1998 1045 2572 9882 1012 2043 2017 1005 2128 3788 2091 1996 2395 1010 1998 1037 2158 5363 2000 2131 2115 2449 1010 1998 1996 2111 2008 2017 3113 1010 2215 2000 2330 2017 2039 2066 4234 1010 2017 2031 2000 10236 2115 18001 2007 1037 2502 2417 6812 1010 7110 13512 2053 7680 7743 2183 2000 7438 2033 2066 1037 7570 1010 1045 1005 1049 1037 2465 2100 6861 3610 2100 8549 6292 2293 2100 10855 2100 17276 4615 1012 2138 2017 1005 2128 18294 18294 1010 1051 11631 1010 1998 1045 1005 1049 9882 9882 1010 2138 2017 1005 2128 18294 18294 1010 1051 11631 1010 1998 1045 1005 1049 9882 9882 1010 2017 1005 2128 19424 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: happy (id = 2)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: happy (id = 2)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] pistol grip pump on my lap at all times , pistol grip pump on my lap at all times , pistol grip pump on my lap at all times , they can be fucking with other ni ##gga ##s ' shit , but they can ' t be fucking with mine , pistol grip pump on my lap at all times , pistol grip pump on my lap at all times , pistol grip pump on my lap at all times , they can be fucking with other ni ##gga ##s ' shit , but they can ' t be fucking with mine . i was raised in the hood called what the di ##ff , where the brothers in the hood be chi ##val ##rous [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] pistol grip pump on my lap at all times , pistol grip pump on my lap at all times , pistol grip pump on my lap at all times , they can be fucking with other ni ##gga ##s ' shit , but they can ' t be fucking with mine , pistol grip pump on my lap at all times , pistol grip pump on my lap at all times , pistol grip pump on my lap at all times , they can be fucking with other ni ##gga ##s ' shit , but they can ' t be fucking with mine . i was raised in the hood called what the di ##ff , where the brothers in the hood be chi ##val ##rous [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 2027 2064 2022 8239 2007 2060 9152 23033 2015 1005 4485 1010 2021 2027 2064 1005 1056 2022 8239 2007 3067 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 2027 2064 2022 8239 2007 2060 9152 23033 2015 1005 4485 1010 2021 2027 2064 1005 1056 2022 8239 2007 3067 1012 1045 2001 2992 1999 1996 7415 2170 2054 1996 4487 4246 1010 2073 1996 3428 1999 1996 7415 2022 9610 10175 13288 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 2027 2064 2022 8239 2007 2060 9152 23033 2015 1005 4485 1010 2021 2027 2064 1005 1056 2022 8239 2007 3067 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 8779 6218 10216 2006 2026 5001 2012 2035 2335 1010 2027 2064 2022 8239 2007 2060 9152 23033 2015 1005 4485 1010 2021 2027 2064 1005 1056 2022 8239 2007 3067 1012 1045 2001 2992 1999 1996 7415 2170 2054 1996 4487 4246 1010 2073 1996 3428 1999 1996 7415 2022 9610 10175 13288 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: angry (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: angry (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] let it rain , let it rain . open the flood ##gate ##s of heaven . repeat . . the lord reigns , let the earth be glad . let the distance shores re ##jo ##ice . clouds and thick that ##rk ##ness surround him . righteous ##ness and justice are the f ##ount ##hat ##tion of his throne . a fire goes before him and consume ##s his foe ##s on every side . his lightning lights up the world . the earth sees and tremble ##s . the mountains melt like wax before the lord . before the lord of all the earth . the heavens pro ##claim his righteous ##ness . and all peoples will see his glory . . we want to [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] let it rain , let it rain . open the flood ##gate ##s of heaven . repeat . . the lord reigns , let the earth be glad . let the distance shores re ##jo ##ice . clouds and thick that ##rk ##ness surround him . righteous ##ness and justice are the f ##ount ##hat ##tion of his throne . a fire goes before him and consume ##s his foe ##s on every side . his lightning lights up the world . the earth sees and tremble ##s . the mountains melt like wax before the lord . before the lord of all the earth . the heavens pro ##claim his righteous ##ness . and all peoples will see his glory . . we want to [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2292 2009 4542 1010 2292 2009 4542 1012 2330 1996 7186 5867 2015 1997 6014 1012 9377 1012 1012 1996 2935 23481 1010 2292 1996 3011 2022 5580 1012 2292 1996 3292 13312 2128 5558 6610 1012 8044 1998 4317 2008 8024 2791 15161 2032 1012 19556 2791 1998 3425 2024 1996 1042 21723 12707 3508 1997 2010 6106 1012 1037 2543 3632 2077 2032 1998 16678 2015 2010 22277 2015 2006 2296 2217 1012 2010 7407 4597 2039 1996 2088 1012 1996 3011 5927 1998 20627 2015 1012 1996 4020 14899 2066 13844 2077 1996 2935 1012 2077 1996 2935 1997 2035 1996 3011 1012 1996 17223 4013 25154 2010 19556 2791 1012 1998 2035 7243 2097 2156 2010 8294 1012 1012 2057 2215 2000 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2292 2009 4542 1010 2292 2009 4542 1012 2330 1996 7186 5867 2015 1997 6014 1012 9377 1012 1012 1996 2935 23481 1010 2292 1996 3011 2022 5580 1012 2292 1996 3292 13312 2128 5558 6610 1012 8044 1998 4317 2008 8024 2791 15161 2032 1012 19556 2791 1998 3425 2024 1996 1042 21723 12707 3508 1997 2010 6106 1012 1037 2543 3632 2077 2032 1998 16678 2015 2010 22277 2015 2006 2296 2217 1012 2010 7407 4597 2039 1996 2088 1012 1996 3011 5927 1998 20627 2015 1012 1996 4020 14899 2066 13844 2077 1996 2935 1012 2077 1996 2935 1997 2035 1996 3011 1012 1996 17223 4013 25154 2010 19556 2791 1012 1998 2035 7243 2097 2156 2010 8294 1012 1012 2057 2215 2000 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: relaxed (id = 3)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: relaxed (id = 3)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 515\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 515\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the that ##y our house collapsed , i went down stream . , i followed the swans , like i follow my dreams . oh ! i was living on borrowed time in a , borrowed house for a borrowed crime . , in need of help i came to your door . , saw the spike of the railing ##s from , the 28 / 3rd floor . , singing , \" build your castle , stop collecting stones , and the river bed shall not be yo ##ru home \" . to the lighthouse my friend ! , i bless your words and education , to the lighthouse my friend . , just go ! just go ! , to the lighthouse my friend [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the that ##y our house collapsed , i went down stream . , i followed the swans , like i follow my dreams . oh ! i was living on borrowed time in a , borrowed house for a borrowed crime . , in need of help i came to your door . , saw the spike of the railing ##s from , the 28 / 3rd floor . , singing , \" build your castle , stop collecting stones , and the river bed shall not be yo ##ru home \" . to the lighthouse my friend ! , i bless your words and education , to the lighthouse my friend . , just go ! just go ! , to the lighthouse my friend [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2008 2100 2256 2160 7798 1010 1045 2253 2091 5460 1012 1010 1045 2628 1996 26699 1010 2066 1045 3582 2026 5544 1012 2821 999 1045 2001 2542 2006 11780 2051 1999 1037 1010 11780 2160 2005 1037 11780 4126 1012 1010 1999 2342 1997 2393 1045 2234 2000 2115 2341 1012 1010 2387 1996 9997 1997 1996 15747 2015 2013 1010 1996 2654 1013 3822 2723 1012 1010 4823 1010 1000 3857 2115 3317 1010 2644 9334 6386 1010 1998 1996 2314 2793 4618 2025 2022 10930 6820 2188 1000 1012 2000 1996 10171 2026 2767 999 1010 1045 19994 2115 2616 1998 2495 1010 2000 1996 10171 2026 2767 1012 1010 2074 2175 999 2074 2175 999 1010 2000 1996 10171 2026 2767 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2008 2100 2256 2160 7798 1010 1045 2253 2091 5460 1012 1010 1045 2628 1996 26699 1010 2066 1045 3582 2026 5544 1012 2821 999 1045 2001 2542 2006 11780 2051 1999 1037 1010 11780 2160 2005 1037 11780 4126 1012 1010 1999 2342 1997 2393 1045 2234 2000 2115 2341 1012 1010 2387 1996 9997 1997 1996 15747 2015 2013 1010 1996 2654 1013 3822 2723 1012 1010 4823 1010 1000 3857 2115 3317 1010 2644 9334 6386 1010 1998 1996 2314 2793 4618 2025 2022 10930 6820 2188 1000 1012 2000 1996 10171 2026 2767 999 1010 1045 19994 2115 2616 1998 2495 1010 2000 1996 10171 2026 2767 1012 1010 2074 2175 999 2074 2175 999 1010 2000 1996 10171 2026 2767 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: sad (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: sad (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] carr ##ion my name , for those who choose to mouth the curse , a tragic serena ##de , with ju ##tha ##ts in my stride , the gothic halls of shame , where statues coldly hold no worse , than the murders i reclaim , from a that ##rk , for ##sa ##ken time . kissing heaven , spent , he wipe ##s lips free of his he ##ctic discharge , wishing to rep ##ent , for the brute that ravaged free , in slight hands beauty weep ##s , conquest ' s deep method ##ical screwing , hurt repeatedly , like the world wound at his feet . dir ##ge inferno . as it is written , that ##m ##n it , so let [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] carr ##ion my name , for those who choose to mouth the curse , a tragic serena ##de , with ju ##tha ##ts in my stride , the gothic halls of shame , where statues coldly hold no worse , than the murders i reclaim , from a that ##rk , for ##sa ##ken time . kissing heaven , spent , he wipe ##s lips free of his he ##ctic discharge , wishing to rep ##ent , for the brute that ravaged free , in slight hands beauty weep ##s , conquest ' s deep method ##ical screwing , hurt repeatedly , like the world wound at his feet . dir ##ge inferno . as it is written , that ##m ##n it , so let [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 12385 3258 2026 2171 1010 2005 2216 2040 5454 2000 2677 1996 8364 1010 1037 13800 14419 3207 1010 2007 18414 8322 3215 1999 2026 18045 1010 1996 7788 9873 1997 9467 1010 2073 11342 24745 2907 2053 4788 1010 2084 1996 9916 1045 24104 1010 2013 1037 2008 8024 1010 2005 3736 7520 2051 1012 7618 6014 1010 2985 1010 2002 13387 2015 2970 2489 1997 2010 2002 13306 11889 1010 10261 2000 16360 4765 1010 2005 1996 26128 2008 25537 2489 1010 1999 7263 2398 5053 27874 2015 1010 9187 1005 1055 2784 4118 7476 29082 1010 3480 8385 1010 2066 1996 2088 6357 2012 2010 2519 1012 16101 3351 21848 1012 2004 2009 2003 2517 1010 2008 2213 2078 2009 1010 2061 2292 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 12385 3258 2026 2171 1010 2005 2216 2040 5454 2000 2677 1996 8364 1010 1037 13800 14419 3207 1010 2007 18414 8322 3215 1999 2026 18045 1010 1996 7788 9873 1997 9467 1010 2073 11342 24745 2907 2053 4788 1010 2084 1996 9916 1045 24104 1010 2013 1037 2008 8024 1010 2005 3736 7520 2051 1012 7618 6014 1010 2985 1010 2002 13387 2015 2970 2489 1997 2010 2002 13306 11889 1010 10261 2000 16360 4765 1010 2005 1996 26128 2008 25537 2489 1010 1999 7263 2398 5053 27874 2015 1010 9187 1005 1055 2784 4118 7476 29082 1010 3480 8385 1010 2066 1996 2088 6357 2012 2010 2519 1012 16101 3351 21848 1012 2004 2009 2003 2517 1010 2008 2213 2078 2009 1010 2061 2292 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: angry (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: angry (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i want a girl with a mind like a diamond , i want a girl who knows what ' s best , i want a girl with shoes that cut , and - ah eyes that burn like cigarettes , ah . i want a girl with the right allocation ##s , who is fast and thorough and sharp as a tack , she ' s playing with her jewelry , she ' s putting up her hair , she ' s touring the facility and picking up slack , i want a girl with a short skirt and a long jacket . i want a girl who gets up early gets up early , i want a girl who stays up late stays up late [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i want a girl with a mind like a diamond , i want a girl who knows what ' s best , i want a girl with shoes that cut , and - ah eyes that burn like cigarettes , ah . i want a girl with the right allocation ##s , who is fast and thorough and sharp as a tack , she ' s playing with her jewelry , she ' s putting up her hair , she ' s touring the facility and picking up slack , i want a girl with a short skirt and a long jacket . i want a girl who gets up early gets up early , i want a girl who stays up late stays up late [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2215 1037 2611 2007 1037 2568 2066 1037 6323 1010 1045 2215 1037 2611 2040 4282 2054 1005 1055 2190 1010 1045 2215 1037 2611 2007 6007 2008 3013 1010 1998 1011 6289 2159 2008 6402 2066 15001 1010 6289 1012 1045 2215 1037 2611 2007 1996 2157 16169 2015 1010 2040 2003 3435 1998 16030 1998 4629 2004 1037 26997 1010 2016 1005 1055 2652 2007 2014 11912 1010 2016 1005 1055 5128 2039 2014 2606 1010 2016 1005 1055 6828 1996 4322 1998 8130 2039 19840 1010 1045 2215 1037 2611 2007 1037 2460 9764 1998 1037 2146 6598 1012 1045 2215 1037 2611 2040 4152 2039 2220 4152 2039 2220 1010 1045 2215 1037 2611 2040 12237 2039 2397 12237 2039 2397 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2215 1037 2611 2007 1037 2568 2066 1037 6323 1010 1045 2215 1037 2611 2040 4282 2054 1005 1055 2190 1010 1045 2215 1037 2611 2007 6007 2008 3013 1010 1998 1011 6289 2159 2008 6402 2066 15001 1010 6289 1012 1045 2215 1037 2611 2007 1996 2157 16169 2015 1010 2040 2003 3435 1998 16030 1998 4629 2004 1037 26997 1010 2016 1005 1055 2652 2007 2014 11912 1010 2016 1005 1055 5128 2039 2014 2606 1010 2016 1005 1055 6828 1996 4322 1998 8130 2039 19840 1010 1045 2215 1037 2611 2007 1037 2460 9764 1998 1037 2146 6598 1012 1045 2215 1037 2611 2040 4152 2039 2220 4152 2039 2220 1010 1045 2215 1037 2611 2040 12237 2039 2397 12237 2039 2397 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: happy (id = 2)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: happy (id = 2)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] c . berry . i said the joint was rocking , going round and round , yeah , reeling and a rocking , what a crazy sound , and they never stopped rocking , ' til the moon went down . well it sounds so sweet , i had to take me chance , rose out of my seat , i just had to that ##nce , started moving my feet , whoa to clapping my hands . i said the joint was a rocking , going round and round , yeah , reeling and a rocking , what a crazy sound , and they never stopped rocking , ' til the moon went down . yeah at twelve o ' clock , yeah the place [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] c . berry . i said the joint was rocking , going round and round , yeah , reeling and a rocking , what a crazy sound , and they never stopped rocking , ' til the moon went down . well it sounds so sweet , i had to take me chance , rose out of my seat , i just had to that ##nce , started moving my feet , whoa to clapping my hands . i said the joint was a rocking , going round and round , yeah , reeling and a rocking , what a crazy sound , and they never stopped rocking , ' til the moon went down . yeah at twelve o ' clock , yeah the place [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1039 1012 10498 1012 1045 2056 1996 4101 2001 14934 1010 2183 2461 1998 2461 1010 3398 1010 28515 1998 1037 14934 1010 2054 1037 4689 2614 1010 1998 2027 2196 3030 14934 1010 1005 18681 1996 4231 2253 2091 1012 2092 2009 4165 2061 4086 1010 1045 2018 2000 2202 2033 3382 1010 3123 2041 1997 2026 2835 1010 1045 2074 2018 2000 2008 5897 1010 2318 3048 2026 2519 1010 23281 2000 27104 2026 2398 1012 1045 2056 1996 4101 2001 1037 14934 1010 2183 2461 1998 2461 1010 3398 1010 28515 1998 1037 14934 1010 2054 1037 4689 2614 1010 1998 2027 2196 3030 14934 1010 1005 18681 1996 4231 2253 2091 1012 3398 2012 4376 1051 1005 5119 1010 3398 1996 2173 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1039 1012 10498 1012 1045 2056 1996 4101 2001 14934 1010 2183 2461 1998 2461 1010 3398 1010 28515 1998 1037 14934 1010 2054 1037 4689 2614 1010 1998 2027 2196 3030 14934 1010 1005 18681 1996 4231 2253 2091 1012 2092 2009 4165 2061 4086 1010 1045 2018 2000 2202 2033 3382 1010 3123 2041 1997 2026 2835 1010 1045 2074 2018 2000 2008 5897 1010 2318 3048 2026 2519 1010 23281 2000 27104 2026 2398 1012 1045 2056 1996 4101 2001 1037 14934 1010 2183 2461 1998 2461 1010 3398 1010 28515 1998 1037 14934 1010 2054 1037 4689 2614 1010 1998 2027 2196 3030 14934 1010 1005 18681 1996 4231 2253 2091 1012 3398 2012 4376 1051 1005 5119 1010 3398 1996 2173 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: happy (id = 2)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: happy (id = 2)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] you got out the wrong side of bed and liked it , the only love he gets is un ##re ##qui ##ted , mourning month ##at ##y all week long ' s not healthy , but being happy ' s only for the wealthy . you can ' t see past the worst case scenario , you ' d be happier instead if you ' d stayed in bed , i pray one that ##y i ' ll live to see you break a smile . wall ##ow in your miserable mess as you tend to , you ' ve mastered looking un ##im ##pressed , do i off ##end you ? mourning month ##at ##y all week long ' s not healthy , but being [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] you got out the wrong side of bed and liked it , the only love he gets is un ##re ##qui ##ted , mourning month ##at ##y all week long ' s not healthy , but being happy ' s only for the wealthy . you can ' t see past the worst case scenario , you ' d be happier instead if you ' d stayed in bed , i pray one that ##y i ' ll live to see you break a smile . wall ##ow in your miserable mess as you tend to , you ' ve mastered looking un ##im ##pressed , do i off ##end you ? mourning month ##at ##y all week long ' s not healthy , but being [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2017 2288 2041 1996 3308 2217 1997 2793 1998 4669 2009 1010 1996 2069 2293 2002 4152 2003 4895 2890 15549 3064 1010 16236 3204 4017 2100 2035 2733 2146 1005 1055 2025 7965 1010 2021 2108 3407 1005 1055 2069 2005 1996 7272 1012 2017 2064 1005 1056 2156 2627 1996 5409 2553 11967 1010 2017 1005 1040 2022 19366 2612 2065 2017 1005 1040 4370 1999 2793 1010 1045 11839 2028 2008 2100 1045 1005 2222 2444 2000 2156 2017 3338 1037 2868 1012 2813 5004 1999 2115 13736 6752 2004 2017 7166 2000 1010 2017 1005 2310 15682 2559 4895 5714 19811 1010 2079 1045 2125 10497 2017 1029 16236 3204 4017 2100 2035 2733 2146 1005 1055 2025 7965 1010 2021 2108 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2017 2288 2041 1996 3308 2217 1997 2793 1998 4669 2009 1010 1996 2069 2293 2002 4152 2003 4895 2890 15549 3064 1010 16236 3204 4017 2100 2035 2733 2146 1005 1055 2025 7965 1010 2021 2108 3407 1005 1055 2069 2005 1996 7272 1012 2017 2064 1005 1056 2156 2627 1996 5409 2553 11967 1010 2017 1005 1040 2022 19366 2612 2065 2017 1005 1040 4370 1999 2793 1010 1045 11839 2028 2008 2100 1045 1005 2222 2444 2000 2156 2017 3338 1037 2868 1012 2813 5004 1999 2115 13736 6752 2004 2017 7166 2000 1010 2017 1005 2310 15682 2559 4895 5714 19811 1010 2079 1045 2125 10497 2017 1029 16236 3204 4017 2100 2035 2733 2146 1005 1055 2025 7965 1010 2021 2108 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: happy (id = 2)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: happy (id = 2)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"xl6ATMmFAhdh","colab_type":"code","colab":{}},"source":["def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","  \"\"\"Creates a classification model.\"\"\"\n","\n","  bert_module = hub.Module(\n","      BERT_MODEL_HUB,\n","      trainable=True)\n","  bert_inputs = dict(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids)\n","  bert_outputs = bert_module(\n","      inputs=bert_inputs,\n","      signature=\"tokens\",\n","      as_dict=True)\n","\n","  # Use \"pooled_output\" for classification tasks on an entire sentence.\n","  # Use \"sequence_outputs\" for token-level output.\n","  output_layer = bert_outputs[\"pooled_output\"]\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  # Create our own layer to tune for politeness data.\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","\n","    # Dropout helps prevent overfitting\n","    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    # Convert labels into one-hot encoding\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","    # If we're predicting, we want predicted labels and the probabilities.\n","    if is_predicting:\n","      return (predicted_labels, log_probs)\n","\n","    # If we're train/eval, compute loss between predicted and actual label\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","    return (loss, predicted_labels, log_probs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGP9iT0YApgx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":130},"outputId":"ed2461f8-9ddc-46e8-f314-14fd5d498642","executionInfo":{"status":"error","timestamp":1575776561968,"user_tz":300,"elapsed":587,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}}},"source":["# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","\n","    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","    \n","    # TRAIN and EVAL\n","    if not is_predicting:\n","\n","      (loss, predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      train_op = bert.optimization.create_optimizer(\n","          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","      # Calculate evaluation metrics. \n","      def metric_fn(label_ids, predicted_labels):\n","        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predicted_labels)\n","        recall = tf.metrics.recall(\n","            label_ids,\n","            predicted_labels)\n","        precision = tf.metrics.precision(\n","            label_ids,\n","            predicted_labels) \n","        true_pos = tf.metrics.true_positives(\n","            label_ids,\n","            predicted_labels)\n","        true_neg = tf.metrics.true_negatives(\n","            label_ids,\n","            predicted_labels)   \n","        false_pos = tf.metrics.false_positives(\n","            label_ids,\n","            predicted_labels)  \n","        false_neg = tf.metrics.false_negatives(\n","            label_ids,\n","            predicted_labels)\n","        conf_matr = tf.math.confusion_matrix(\n","            label_ids,\n","            predicted_labels)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"f1_score\": f1_score,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg,\n","            with tf.Session():\n","              print('Confusion Matrix: \\n\\n', tf.Tensor.eval(conf_matr,feed_dict=None, session=None))\n","        }\n","\n","      eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","          loss=loss,\n","          train_op=train_op)\n","      else:\n","          return tf.estimator.EstimatorSpec(mode=mode,\n","            loss=loss,\n","            eval_metric_ops=eval_metrics)\n","    else:\n","      (predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      predictions = {\n","          'probabilities': log_probs,\n","          'labels': predicted_labels\n","      }\n","      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  # Return the actual model function in the closure\n","  return model_fn"],"execution_count":50,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-19aba5b3a7c3>\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    with tf.Session():\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"yCOIDD4fAsYh","colab_type":"code","colab":{}},"source":["# Compute train and warmup steps from batch size\n","# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n","BATCH_SIZE = 16\n","LEARNING_RATE = 2e-6\n","NUM_TRAIN_EPOCHS = 5.0\n","# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E716HC4LA4te","colab_type":"code","colab":{}},"source":["# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uapB9IdA50d","colab_type":"code","outputId":"4b740e73-72ce-433c-ef53-5f250b66b6e4","executionInfo":{"status":"ok","timestamp":1575775683421,"user_tz":300,"elapsed":654,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["model_fn = model_fn_builder(\n","  num_labels=len(label_list),\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps)\n","\n","estimator = tf.estimator.Estimator(\n","  model_fn=model_fn,\n","  params={\"batch_size\": BATCH_SIZE})"],"execution_count":44,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpi7kj7ruf\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpi7kj7ruf\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpi7kj7ruf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe5014d4588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpi7kj7ruf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe5014d4588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ACOjG3uiwt9n","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AEm2c1T6wzUV","colab_type":"code","outputId":"dda4cdc9-f884-4914-8a96-3641d6abdc83","colab":{"base_uri":"https://localhost:8080/","height":921},"executionInfo":{"status":"ok","timestamp":1575776350296,"user_tz":300,"elapsed":663513,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}}},"source":["print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Beginning Training!\n","INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Create CheckpointSaverHook.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Create CheckpointSaverHook.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpi7kj7ruf/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpi7kj7ruf/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.4187671, step = 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.4187671, step = 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.924037\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.924037\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.3444617, step = 100 (108.223 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.3444617, step = 100 (108.223 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10014\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10014\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.1452115, step = 200 (90.898 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.1452115, step = 200 (90.898 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.09836\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.09836\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.9166299, step = 300 (91.044 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.9166299, step = 300 (91.044 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.09359\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.09359\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.91090333, step = 400 (91.443 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.91090333, step = 400 (91.443 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.0955\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.0955\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.83642256, step = 500 (91.287 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.83642256, step = 500 (91.287 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.09983\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.09983\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.86334044, step = 600 (90.921 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.86334044, step = 600 (90.921 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 634 into /tmp/tmpi7kj7ruf/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 634 into /tmp/tmpi7kj7ruf/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 643 into /tmp/tmpi7kj7ruf/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 643 into /tmp/tmpi7kj7ruf/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: 0.71907383.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: 0.71907383.\n"],"name":"stderr"},{"output_type":"stream","text":["Training took time  0:11:02.974540\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YLdMvgUww24B","colab_type":"code","colab":{}},"source":["test_input_fn = run_classifier.input_fn_builder(\n","    features=test_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ffwy6mm3mUYU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":181},"outputId":"37dddf45-a99d-4fa2-f624-6cee4226f641","executionInfo":{"status":"error","timestamp":1575776514767,"user_tz":300,"elapsed":528,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}}},"source":[""],"execution_count":49,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-4fb817d4e33b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confusion Matrix: \\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_matr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'conf_matr' is not defined"]}]},{"cell_type":"code","metadata":{"id":"FCLVDVwyw7Nv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":429},"outputId":"80e1fe46-2042-4adf-bdb8-c35c6149087d","executionInfo":{"status":"error","timestamp":1575776376614,"user_tz":300,"elapsed":13084,"user":{"displayName":"Joseph Mercer","photoUrl":"","userId":"07298541810250475785"}}},"source":["estimator.evaluate(input_fn=test_input_fn, steps=None)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-f6cdcd6190a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    478\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m   def _actual_eval(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_actual_eval\u001b[0;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         (scaffold, update_op, eval_dict, all_hooks) = (\n\u001b[0;32m--> 504\u001b[0;31m             self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n\u001b[0m\u001b[1;32m    505\u001b[0m         return self._evaluate_run(\n\u001b[1;32m    506\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_build_graph\u001b[0;34m(self, input_fn, hooks, checkpoint_path)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m       (scaffold, evaluation_hooks, input_hooks, update_op, eval_dict) = (\n\u001b[0;32m-> 1511\u001b[0;31m           self._call_model_fn_eval(input_fn, self.config))\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn_eval\u001b[0;34m(self, input_fn, config)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m     estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1547\u001b[0;31m         features, labels, ModeKeys.EVAL, config)\n\u001b[0m\u001b[1;32m   1548\u001b[0m     eval_metric_ops = _verify_and_create_loss_metric(\n\u001b[1;32m   1549\u001b[0m         estimator_spec.eval_metric_ops, estimator_spec.loss)\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-41-ffb6cfde1df6>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m     69\u001b[0m           return tf.estimator.EstimatorSpec(mode=mode,\n\u001b[1;32m     70\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             eval_metric_ops=eval_metrics)\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       (predicted_labels, log_probs) = create_model(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/model_fn.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, mode, predictions, loss, train_op, eval_metric_ops, export_outputs, training_chief_hooks, training_hooks, scaffold, evaluation_hooks, prediction_hooks)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mprediction_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_estimator_spec_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mtraining_chief_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_estimator_spec_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_chief_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0meval_metric_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_eval_metric_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_metric_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mscaffold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_scaffold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaffold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/model_fn.py\u001b[0m in \u001b[0;36m_validate_eval_metric_ops\u001b[0;34m(eval_metric_ops)\u001b[0m\n\u001b[1;32m    490\u001b[0m           raise TypeError(\n\u001b[1;32m    491\u001b[0m               \u001b[0;34m'Values of eval_metric_ops must be (metric_value, update_op) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m               'tuples, given: {} for key: {}'.format(value, key))\n\u001b[0m\u001b[1;32m    493\u001b[0m   \u001b[0;31m# Verify all tensors and ops are from default graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m   \u001b[0mdefault_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Values of eval_metric_ops must be (metric_value, update_op) tuples, given: Tensor(\"confusion_matrix/SparseTensorDenseAdd:0\", shape=(?, ?), dtype=int32) for key: confusion_matrix"]}]}]}